{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847a4931-52b3-4a24-b231-e4b40db3cec5",
   "metadata": {},
   "source": [
    "# ISPRS Part 4\n",
    "\n",
    "This notebook is a glimpse into machine learning through the ESDL platform and `xcube`.\n",
    "\n",
    "The idea is to have a very simple model to predict the land surface temperature from air temperature 2m in Australia. As in this tutorial we do not have any available GPU, we have to limit ourselves to such a simple model and only show the general workflow.\n",
    "\n",
    "**Please don't expect great results from a very simple approach like this**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8052c-4823-49b1-aefa-4a68eaa052de",
   "metadata": {},
   "source": [
    "First, we load all the libraries packages. Especially the package `ml4xcube` has a leading role here and is the interface between `xcube` and `pytorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed3accf-be29-4bcf-83ed-3f03c0a2ec38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import dask.array as da\n",
    "from torch import nn\n",
    "from global_land_mask import globe\n",
    "from xcube.core.store import new_data_store\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from ml4xcube.cube_utilities import get_chunk_sizes\n",
    "from ml4xcube.datasets.xr_dataset import XrDataset\n",
    "from ml4xcube.statistics import get_range, get_statistics, standardize, standardize, undo_standardizing\n",
    "from ml4xcube.datasets.pytorch_xr import prepare_dataloader\n",
    "from ml4xcube.training.pytorch import Trainer\n",
    "from ml4xcube.data_assignment import assign_block_split, assign_rand_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c165b23f-eff8-4e85-acee-e277ba58fe41",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972b8434-c3dc-4d42-9221-6934c2f05e44",
   "metadata": {},
   "source": [
    "We will use another datacube here to get access to the land surface temperature which does not completely cover the world map and has gaps. Also the data is only available until the end of 2011. So we will concentrate on the time between 2009 and 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a6be6c-2f1e-4fc3-9fd3-e031ff6f8e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_store = new_data_store(\"s3\", root=\"esdl-esdc-v2.1.1\", storage_options=dict(anon=True))\n",
    "dataset    = data_store.open_data('esdc-8d-0.083deg-1x2160x4320-2.1.1.zarr')\n",
    "\n",
    "# Limit to spatial and temporal dimensions\n",
    "aus_data = dataset[['land_surface_temperature', 'air_temperature_2m']].sel(\n",
    "    lat = slice(-17, -37),\n",
    "    lon = slice(110, 130),\n",
    "    time = slice('2009-01-01', '2011-12-31')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d12ef-c9bf-451c-88b1-a840ec9cfc60",
   "metadata": {},
   "source": [
    "Land surface temperature only exist on land. So we want to add a land mask to our dataset. The stacking of data which is not time-dependend is not necessary in reality, but it makes things much easier when accessing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043e9372-dd04-4b41-9bf5-805561f0dcf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lon_grid, lat_grid = np.meshgrid(aus_data.lon,aus_data.lat)\n",
    "lm0                = da.from_array(globe.is_land(lat_grid, lon_grid))\n",
    "lm = da.stack([lm0 for i in range(aus_data.sizes['time'])], axis = 0)\n",
    "extended_data = aus_data.assign(\n",
    "    land_mask = (['time','lat','lon'], lm)\n",
    ")\n",
    "extended_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1487914d-8cfc-4408-974e-6955051ecb44",
   "metadata": {},
   "source": [
    "Let's have a look at the data. You can clearly see the gabs in the the land surface temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc5fb7a-8a04-46bc-a9f7-4df71278c8c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataslice = extended_data.sel(time='2009-08-25')\n",
    "fig = plt.figure(figsize=(6,10))\n",
    "ax1 = fig.add_subplot(211)\n",
    "dataslice['air_temperature_2m'].plot()\n",
    "ax2 = fig.add_subplot(212)\n",
    "dataslice['land_surface_temperature'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcb03a9-7628-4936-974e-007eb18cd3e0",
   "metadata": {},
   "source": [
    "For machine learning approaches, we need to split the data into a training and a test dataset. This can be done with `ml4xcube`. There are two possible ways: random sampling and block sampling. While random sampling is the standard for less **autocorrelated** data, it is recommended to use block sampling in spatio-temporal data to overcome **overfitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26efefd-939d-4f14-8446-e878e4445258",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#random sampling\n",
    "#final_data = assign_rand_split(\n",
    "#    ds    = extended_data,\n",
    "#    split = 0.8\n",
    "#)\n",
    "\n",
    "# block sampling\n",
    "final_data = assign_block_split(\n",
    "    ds    = extended_data,\n",
    "    block_size = [(\"time\", 20), (\"lat\", 20), (\"lon\", 20)],\n",
    "    split = 0.8\n",
    ")\n",
    "final_data.split.sel(time = '2009-08-25').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36396f45-a9ac-4004-974d-dd6b06da5250",
   "metadata": {},
   "source": [
    "We now have a nice dataset which is ready to be used in machine learning approaches of different possible frameworks. In our case we decided to use **PyTorch**. So from now we have some framework agnostic code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37527306-8f34-43a4-a554-a9617d47d6c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare the data for PyTorch\n",
    "\n",
    "The PyTorch framework is well known and a classic approach in machine learning applications. As we are limited by computational power, we will not go into deep learning, as no GPU is present on this Jupyterlab. In general it is easy to apply for a GPU when using DeepESDL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacecc79-1398-4dff-8e20-d968041e850f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc62f8c-f60a-4128-bf89-0d066fc49e8a",
   "metadata": {},
   "source": [
    "For machine learning the data needs some more processing. This includes standardization. It means all data are scaled to a common ground. This is a standard procedure in machine learning. Also DataLoader are created, which are essential for PyTorch. It is common practice to use `X` and `Y` here which resembles our data for the regression model $y = f(x)$.\n",
    "\n",
    "Here: Hyperparameter **batch_size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cfa0a1-437c-4216-975b-3cf448deeea7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make XR dataset\n",
    "dataset = XrDataset(ds=final_data, num_chunks=3, rand_chunk=False).get_dataset()\n",
    "# standardize the datasets\n",
    "at_stat  = get_statistics(dataset, 'air_temperature_2m')\n",
    "lst_stat = get_statistics(dataset, 'land_surface_temperature')\n",
    "X = standardize(dataset['air_temperature_2m'], *at_stat)\n",
    "Y = standardize(dataset['land_surface_temperature'], *lst_stat)\n",
    "# split the arrays into Train and Test\n",
    "X_train, X_test = X[dataset['split'] == True], X[dataset['split'] == False]\n",
    "Y_train, Y_test = Y[dataset['split'] == True], Y[dataset['split'] == False]\n",
    "# shaping as inputs\n",
    "X_train = X_train.reshape(-1, 1)  # Making it [num_samples, 1]\n",
    "Y_train = Y_train.reshape(-1, 1)  # Making it [num_samples, 1]\n",
    "X_test  = X_test.reshape(-1, 1)\n",
    "Y_test  = Y_test.reshape(-1, 1)\n",
    "# prepare according PyTorch dataloaders\n",
    "# Hyperparamater batch_size\n",
    "train_ds     = TensorDataset(torch.tensor(X_train), torch.tensor(Y_train))\n",
    "train_loader = prepare_dataloader(train_ds, batch_size=64, parallel=False)\n",
    "test_ds      = TensorDataset(torch.tensor(X_test), torch.tensor(Y_test))\n",
    "test_loader  = prepare_dataloader(test_ds, batch_size=64, parallel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3cf56e-4ffe-4f06-bd05-3940fdca24a5",
   "metadata": {},
   "source": [
    "## Model development\n",
    "\n",
    "The model of a machine learning system is also called architecture. We choose a very shallow network with only 4 linear layers. So we have 1 input layer, 2 hidden layers and 1 output layer. More layers or different architectures might improve the result, but also extend the training times significantly.\n",
    "\n",
    "Here: Hyperparameter **learning rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f3bfd5-88ce-4976-bd2b-638c438e5b11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model, loss and optimizer\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# learning rate\n",
    "lr     = 0.000001\n",
    "epochs = 10\n",
    "\n",
    "reg_model = Model(input_size=1, hidden_size=1, output_size=1)\n",
    "mse_loss  = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(reg_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97090738-3bb2-4c9c-9ffa-6dffdc29e520",
   "metadata": {},
   "source": [
    "Next to the model it is important to configure the Trainer. This is the part where all basic blocks like model, datasets and more things are coming together. As you see, we also include a way to stop learning earlier, if the model didn't improve after 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b30d53-4424-4477-82ac-6e74a38a6e56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path for saving the best model\n",
    "best_model_path = './best_model.pth'\n",
    "\n",
    "# Trainer instance\n",
    "trainer = Trainer(\n",
    "    model           = reg_model,\n",
    "    train_data      = train_loader,\n",
    "    test_data       = test_loader,\n",
    "    optimizer       = optimizer,\n",
    "    best_model_path = best_model_path,\n",
    "    early_stopping  = True,\n",
    "    patience        = 3,\n",
    "    epochs          = epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6958d9ce-b3d0-4c98-b288-f26d1659b069",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "The following line will train the model. This might take some minutes of time. Also this is a statistical process and might change every time you run this. Also please have in mind you need to reset the model, if you want to restart the training. This means you need to run the cells again since you defined the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c10984c-b105-464f-b8ea-30aa9724baec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In case you want to reset all training progress of the model, run this cell\n",
    "\n",
    "for layer in reg_model.children():\n",
    "   if hasattr(layer, 'reset_parameters'):\n",
    "       layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5d55ac-5be3-40ba-83ec-fe747a98c264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "reg_model = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd530d-9980-4947-83fa-ff12eef2b358",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Now it's time to find out about the results of the model. We will plot the result for one single point in time. So we pick all the points and need to transform them to fit the model, like we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9a7cad-999c-4f70-ba5d-54a2f7227ab1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_df     = final_data.sel(time = '2009-08-25').to_dataframe()\n",
    "result_df_lm  = result_df[result_df['land_mask'] == True]\n",
    "orig          = result_df_lm.dropna()\n",
    "to_pred = result_df_lm[np.isnan(result_df_lm['land_surface_temperature'])]\n",
    "output  = to_pred.drop('land_surface_temperature', axis = 1)\n",
    "X = standardize(to_pred['air_temperature_2m'], *at_stat)\n",
    "X = X.values\n",
    "# Ensure X is a float32 tensor\n",
    "X_tensor = torch.tensor(X.reshape(-1, 1), dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c72a0e6-6beb-47e5-8075-15a8ddc4fe3d",
   "metadata": {},
   "source": [
    "This is the cell where the model is applied to the data to find the proper result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdde88e-911f-4b17-a165-df8099a714c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the tensor with the correct dtype when calling the model\n",
    "lstp = reg_model(X_tensor)\n",
    "output['land_surface_temperature'] = undo_standardizing(lstp.detach().cpu().numpy(), *lst_stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae58c985-191b-442d-86e4-f00aa25a4101",
   "metadata": {},
   "source": [
    "To plot the final results, we need the result as xarray dataarrays again. This is a cumbersome process which will be better addressed in a future release of `ml4xcube`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45697de-b534-4017-84c0-c277d4d5c21d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get data back to xarray\n",
    "df  = pd.concat([orig['land_surface_temperature'], output['land_surface_temperature']])\n",
    "series_aggregated = df.groupby(['lat', 'lon']).mean()\n",
    "df_reshaped = series_aggregated.unstack()\n",
    "data_array = xr.DataArray(\n",
    "    df_reshaped.values,  # 2D array of the land surface temperature values\n",
    "    coords={'lat': df_reshaped.index.values, 'lon': df_reshaped.columns.values},  # lat and lon as coordinates\n",
    "    dims=['lat', 'lon']  # Specify dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10afbde-a657-4969-9768-8083cd31ec18",
   "metadata": {},
   "source": [
    "## Result plot\n",
    "\n",
    "Now it comes to the last cell of the workshop. You've seen how easy it is to use machine learning models with this system. The whole process does not change for complex deep learning models. The pipeline is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617d418c-f37e-494e-932a-a02349754c68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = final_data.sel(time='2009-08-25')\n",
    "fig = plt.figure(figsize=(6,10))\n",
    "ax1 = fig.add_subplot(311)\n",
    "(df['air_temperature_2m']).plot()\n",
    "ax2 = fig.add_subplot(312)\n",
    "(df['land_surface_temperature']).plot()\n",
    "ax3 = fig.add_subplot(313)\n",
    "data_array.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7afb90-a9d7-4b54-a0ae-aba6d1fa7668",
   "metadata": {},
   "source": [
    "The result might seem that there is no difference in the predicitons, but there are. The model is limited by design. Other models with longer computation times will get much better results here. Especially when time-series or location aspects are used. \n",
    "\n",
    "Some ideas for nice models in spatio temporal analysis are: CNNs, LSTM, Transformers and mixes of those. Also Autoencoders can give very nice results. Especially when it comes to gap-filling processes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepesdl-ml-2024.05",
   "language": "python",
   "name": "conda-env-deepesdl-ml-2024.05-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
